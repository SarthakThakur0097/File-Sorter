import os, shutil

path = r"C:\Users\Sarthak\Documents\ML\File-Sorter/"

os.listdir(path)


folder_names = ['csv files', 'image files', 'text files']
for loop in range(0, 3):
    if not os.path.exists(path + folder_names[loop]):
        os.makedirs((path+ folder_names[loop]))


files = os.listdir(path)

for file in files:
    if ".csv" in file and not os.path.exists(path+f"csv files/{file}"):
        shutil.move(path+file, path + f"csv files/{file}")
    elif ".jpg" in file and not os.path.exists(path+f"image files/{file}"):
        shutil.move(path+file, path + f"image files/{file}")
    elif ".txt" in file and not os.path.exists(path+f"text files/{file}"):
        shutil.move(path+file, path + f"text files/{file}")


from bs4 import BeautifulSoup
import requests


url = "https://en.wikipedia.org/wiki/List_of_largest_companies_in_the_United_States_by_revenue"

page = requests.get(url)


table = soup.find('table', class_="wikitable sortable")


world_titles = table.find_all('th')
world_titles


world_table_titles = [title.text.strip() for title in world_titles]


print(world_table_titles)


import pandas as pd


df = pd.DataFrame(columns=world_table_titles)


df


column_data = table.find_all('tr')


for row in column_data[1:]:
    row_data = row.find_all('td')
    individual_row_data = [data.text.strip() for data in row_data]
   
    length = len(df)
    df.loc[length] = individual_row_data


df


import requests
import smtplib
import time
import datetime

# Connect to website

URL = "https://www.youtube.com/watch?v=K7cdNN4lvYI"
headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36"}

page = requests.get(URL, headers=headers)

soup1 = BeautifulSoup(page.content, "html.parser")

soup2 = BeautifulSoup(soup1.prettify(), "html.parser")



from selenium import webdriver
import time
from selenium.webdriver.common.by import By

# Start a Chrome browser session
driver = webdriver.Chrome()

# Open the YouTube video URL
video_url = "https://www.youtube.com/watch?v=K7cdNN4lvYI"
driver.get(video_url)

# Maximize the browser window (optional)
driver.maximize_window()

# Scroll down to load the video description and comments
for _ in range(3):  # You can adjust the number of times to scroll
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(2)  # Adjust the sleep time as needed

# Wait for some time to ensure that the video and comments are fully loaded
time.sleep(10)  # Adjust the sleep time as needed

# Get the full page HTML
page_html = driver.page_source

# Find the video title element in the page source
start_index = page_html.find_element(By.CLASS_NAME,'yt-spec-touch-feedback-shape__fill')
start_index.click()


from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By

# Set the path to the Chrome WebDriver executable
webdriver_path = 'path_to_chromedriver.exe'  # Replace with the actual path

# Create a Chrome WebDriver service with the executable path
service = Service(webdriver_path)

# Create Chrome WebDriver with the service
driver = webdriver.Chrome(service=service)

# Navigate to the YouTube video URL
video_url = 'https://www.youtube.com/watch?v=your_video_id_here'
driver.get(video_url)

# Locate the yt-formatted-string element with the video title
video_title_element = driver.find_element(By.XPATH, "//yt-formatted-string[contains(@class, 'ytd-watch-metadata')]")
video_title = video_title_element.text

# Print the video title
print("Video Title:", video_title)

# Close the WebDriver
driver.quit()



import subprocess
import os

def download_video(url, output_dir="./"):
    # Construct the command for YouTube-DL
    command = ["youtube-dl", url, "-o", os.path.join(output_dir, "%(title)s.%(ext)s")]

    try:
        # Run YouTube-DL using subprocess
        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

        # Check if the download was successful
        if result.returncode == 0:
            print("Download successful!")
        else:
            print("Download failed.")
            print(result.stderr)

    except Exception as e:
        print("An error occurred:", str(e))

# Example usage:
video_url = "https://www.youtube.com/watch?v=K7cdNN4lvYI"
download_video(video_url)



def read_file_to_string(file_path, encoding='utf-8'):
    try:
        with open(file_path, 'r', encoding=encoding) as file:
            file_contents = file.read()
        return file_contents
    except FileNotFoundError:
        return "File not found."
    except Exception as e:
        return f"An error occurred: {str(e)}"

# Example usage:
file_path = r'C:\Users\Sarthak\Documents\ML\File-Sorter\text files\holland.txt'
file_contents = read_file_to_string(file_path)





import re

def split_and_clean_string(input_string):
    # Remove numbers using regular expressions
    cleaned_string = re.sub(r'\d+', '', input_string)

    # Remove extra spaces and newlines, then replace colons with spaces
    cleaned_string = ' '.join(cleaned_string.split())
    cleaned_string = cleaned_string.replace(':', '')

    # Split the cleaned string into smaller strings, each with a maximum length of 2048
    max_length = 2048
    split_strings = [cleaned_string[i:i+max_length] for i in range(0, len(cleaned_string), max_length)]

    return split_strings

# Example usage:
input_string = """This is a long string with numbers: 12345. It contains multiple lines
and even some colons: like this: one and this: two. The string should be split into smaller
parts, each with a maximum length of 2048 characters. Let's see how it works."""

split_strings = split_and_clean_string(file_contents)

print(split_strings[0])



import re

def clean_string(input_string):
    # Remove numbers using regular expressions
    cleaned_string = re.sub(r'\d+', '', input_string)
    
    # Remove extra spaces and newlines, then replace colons with spaces
    cleaned_string = ' '.join(cleaned_string.split())
    cleaned_string = cleaned_string.replace(':', '')
    
    return cleaned_string

# Example usage:
input_string = ""

cleaned_output = clean_string(input_string)
print(cleaned_output)



import yt_dlp

URLS = ['https://www.youtube.com/watch?v=K7cdNN4lvYI']

def longer_than_a_minute(info, *, incomplete):
    """Download only videos longer than a minute (or with unknown duration)"""
    duration = info.get('duration')
    if duration and duration < 60:
        return 'The video is too short'

ydl_opts = {
    'match_filter': longer_than_a_minute,
}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    error_code = ydl.download(URLS)





print(soup.find(class_="segment-text").get_text())


import datetime
import csv

def check_price():
    #URL = "https://www.amazon.com/Compression-Athletic-Baselayer-Undershirt-Workout-4/dp/B0B51MKCFJ/?_encoding=UTF8&pd_rd_w=tel1C&content-id=amzn1.sym.e1d34ef2-481b-4c3a-8455-1d9cdc0cb942&pf_rd_p=e1d34ef2-481b-4c3a-8455-1d9cdc0cb942&pf_rd_r=66XGQSRMAVZ30X58ACT6&pd_rd_wg=snrml&pd_rd_r=5c8aa8c4-1bad-4983-a88c-4065e4a430ec&ref_=pd_gw_bmx_gp_jg4vc6pn"
    URL = "https://www.indeed.com/jobs?q=Junior+Data+Analyst&l=New+York%2C+NY&from=mobRdr&utm_source=%2Fm%2F&utm_medium=redir&utm_campaign=dt"
    URL = "https://www.walmart.com/ip/onn-75-Class-4K-UHD-2160P-LED-Frameless-Roku-Smart-TV-100044717/656435841?athbdg=L1300"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36"}

    soup1 = BeautifulSoup(requests.get(URL, headers=headers).text , "html.parser")
    soup2 = BeautifulSoup(soup1.prettify(), "html.parser")
    print(soup2)
    price = soup.find(class_="lh-copy").get_text()
    #title = soup2.find(id='productTitle').get_text()
    price = price.strip()[1:]
    title = title.strip()
    today = datetime.date.today()
    
    print(today)
    
    header = ['Title', 'Price', 'date']
    data = [title, price, today]

    with open('AmazonWebScraperDataset.csv', 'a+', newline='', encoding='UTF-8') as f:
        writer = csv.writer(f)
        writer.writerow(data)


while(True):
    check_price()
    time.sleep(5)


import pandas as pd

df = pd.read_csv('C:\\Users\\Sarthak\\Documents\\ML\\AmazonWebScraperDataset.csv')

df


import collections

# Sample job descriptions (replace with your actual data)
job_descriptions = [
    "This job requires strong Python programming skills.",
    "The candidate should have experience with data analysis tools.",
    "We are looking for a candidate with SQL expertise.",
]

# Combine job descriptions into a single text
combined_text = " ".join(job_descriptions)

# Tokenize the text (split it into words or tokens)
tokens = combined_text.split()

# Calculate term frequency using collections.Counter
term_frequency = collections.Counter(tokens)

# Display term frequency for the top N terms (change N as needed)
top_terms = term_frequency.most_common(10)
for term, frequency in top_terms:
    print(f"{term}: {frequency}")



